課題はない。

機械学習
・強化学習
　・Ｑ学習

学習ってなんだろう？
広辞苑によると
がく-しゅう【学習】
①まなびならうこと
②過去の経験の上に立って、新しい知識や技術を習得すること。広義には精神・身体の後天的発達を言う。
③行動が経験によって多少とも持続的に変容を示すこと。

教師付き学習と教師なし学習
・教師付き学習
教師が学習者にどのようにするか（解）を提示

・教師なし学習
学習者が試行錯誤して解を見つけ出す。

なしのほうが汎用性がたかい。
例えば、塗装するロボットがいるとする。
　教師付き学習だと、熟練工の人がロボットのアームの先端を持ってどこを塗装するのか教える。ところがこれだと、塗装するものの位置がずれていたらエラー、塗装するものの形が違っていたらエラー、になり再学習が必要になる。つまり、教師付き学習は汎用性が低い。
　どのような出力をしなさいという明示的な出力が与えられ、学習者の出力がこれに近づくように学習者が学習をする枠組み。

　教師なし学習は、解の求め方を教えるのではないので、人間が思ってもないような解の導き方を見つけることがある。
　明示的な出力の提示はないが、学習者が正しい出力を行えば、環境から報酬をもらうことができる。悪い出力を行えば、環境から罰をもらうこともある。


　例、インベーダーゲーム、ブロック崩しであれば、スコアを報酬としてこれを多くするようにする。

　有名な例として
スキナーボックス
・はじめ、ネズミは箱の中を動き回る
・たまたま、ネズミがレバーを押すと餌が出てくる
・何度か同じ行動を繰り返す
・そのうちレバーを押すと餌が出てくることを覚えて頻繁にレバーを押すようになる

迷路問題
・１．ランダムに行動
・２．たまたまゴールにたどり着き報酬をもらう
・３.報酬を伝搬していく
・４．なんども繰り返すことでゴールへ最短距離で行けるようになる
減衰させる、０．９などをかける

報酬の伝搬の仕方
ゴールまでの手順を覚えておき、ゴールからスタートまで報酬を減衰させていく方法。
　これを繰り返していくと、報酬の累積値が変わっていき、累積値の大きい方に進めていくとより多くの報酬が得られる。

例、プロフィットシェアリング（利益分配）
この方法は、最適解にたどり着くという証明はされていない。


Ｑ学習
行動をするごとに学習をする（一つ進むごとに学習をする）伝搬させる
スタートからゴールまでの経路を覚えなくてもいい。
壁にあたった時、行動したけれど場所が変わらない。

行動価値・・どこの場所でどう動いたらその価値はいくつかを表す

PSもQ学習も行動価値で

報酬
Ｑ学習）ワンステップ動いた時まえいた場所と変わって分配されるもの。即時報酬。

価値
あるばしょでその行動をした時にその後の行動をし続けるとエージェントがもらえる報酬の合計値


価値のほうが重要


リバーシの例）
報酬は増えたコマの数
価値は将来もらえる報酬、と考えると価値というものが大事。



迷路問題
Ｑ値・・・場所Ａのマスの中の状態
・Ｑ（A,　上）＝１２０
・Ｑ（A,　下）＝１
・Ｑ（A,　右）＝５００
・Ｑ（A,　左）＝１０

学習方法（Ｑ値の更新）
Ｑi+1（場所、行動）　＜＝　Ｑi(次の場所、行動）　
＋α[r　＋　γ　＊　maxＱi（次の場所、　行動) - Qi(場所、行動）］

iはその場所に到達した回数
rはゴールに達した時のみ１００それ以外は０

γ・・・減衰値
γがないと長いルートと短いルートの報酬が同じになってしまう。短いルートのほうが価値はたかいよね。

ｒとγは０より大きく１より小さいことが収束の条件


この学習方法で学習できるのか（収束条件）

ここ　と　ここ　の差を0にするようにする式
a < 1, γ < 1　にすればＱの値はi => ∞ 




Ｑ学習
・学習時の行動の仕方
【探査】　　・ランダム選択・・・Ｑ値は無視、一番簡単だが効率は悪い
【知識利用】・グリーディ選択（よくばり選択）・・・Ｑ値が一番大きな行動を常に選択（一度０よ
	　り大きい値が入るとそれ以外の行動を取らないため、学習が進まない）
　　
・知識利用と探査のバランスが重要
知識利用だけだと最初に通ったものしか行かなくなり、探査だけだと時間がかかる。

・望ましい行動選択
知識利用と探査の両方を行う

・ε-グリーディ選択
確率εでランダムに行動を選択
確率１ーεでグリーディ選択
0 <= ε　<= 1
0の時はグリーディ選択
1の時はランダム選択

例）行動数４
・それぞれがε/4で選ばれる
・グリーディ行動は1-εで選ばれる
・グリーディ行動：(1-ε)+ (ε/4)
・残りの行動：ε/4

・ルーレット選択
Ｑ値を用いて選択
初期値が０だとグリーディ選択
初期値は０より大きな値ではくては行けない

オプティミスティック初期値
オプティミスティック・・・楽観的
初期値を０よりかなり大きな値を入れておく、すると次の状態から報酬をもらってきた値は初期値よりも小さくなる
グリーディ選択でもすべて回る



ランダムな動作をするには乱数を発生させなくてはならない、乱数の発生にはムラがあるので、発生の回数が少なければ少ないほど、学習が収束する。



ボルツマン選択
・シュミレーテッドアニーリング
アニーリング・・・冶金
日本等の精錬の仕方・・・熱して、冷やす、熱して、冷やす
なぜ、何度も？、温度を上げると分子が自由に動きまわる、叩くと結晶構造ができる、冷やすと動かなくなる。

温度定数によりランダム選択とグリーディ選択を行き来するようにする

ボルツマン関数


